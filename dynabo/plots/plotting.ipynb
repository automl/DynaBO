{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the CSV files\n",
    "baseline_table = pd.read_csv(\"dynabo/plots/baseline_table.csv\")\n",
    "baseline_incumbent = pd.read_csv(\"dynabo/plots/baseline_incumbent.csv\")\n",
    "\n",
    "dynabo_table = pd.read_csv(\"dynabo/plots/prior_table.csv\")\n",
    "dynabo_incumbent = pd.read_csv(\"dynabo/plots/prior_incumbent.csv\")\n",
    "dynabo_priors = pd.read_csv(\"dynabo/plots/prior_priors.csv\")\n",
    "\n",
    "pibo_table = pd.read_csv(\"dynabo/plots/pibo_table.csv\")\n",
    "pibo_incumbent = pd.read_csv(\"dynabo/plots/pibo_incumbent.csv\")\n",
    "pibo_priors = pd.read_csv(\"dynabo/plots/pibo_priors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynabo_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynabo_incumbent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynabo_priors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(df: pd.DataFrame, incumbents: pd.DataFrame, priors: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    incumbents = incumbents.drop(columns=[\"ID\"])\n",
    "    if priors is not None:\n",
    "        priors = priors.drop(columns=[\"ID\"])\n",
    "        priors = priors[[\"experiment_id\", \"after_n_evaluations\", \"performance\"]]\n",
    "        priors.columns = [\"experiment_id\", \"after_n_evaluations\", \"prior_performance\"]\n",
    "\n",
    "    df = df.merge(incumbents, left_on=\"ID\", right_on=\"experiment_id\")\n",
    "    if priors is not None:\n",
    "        df = df.merge(priors, on=[\"experiment_id\", \"after_n_evaluations\"], how=\"left\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset(baseline_data: pd.DataFrame, dynabo_data: pd.DataFrame, pibo_data: pd.DataFrame, scenario_dataset: List[Tuple[str, str]]):\n",
    "    fig, axs = plt.subplots(len(scenario_dataset), 1, figsize=(10, 10))\n",
    "    for i, (scenario, dataset) in enumerate(scenario_dataset):\n",
    "        plot_run(baseline_data, dynabo_data, pibo_data, scenario, dataset, axs[i])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_run(baseline_data: pd.DataFrame, dynabo_data: pd.DataFrame, pibo_data: pd.DataFrame, scenario: str, dataset: str, prior_kind: str, ax: plt.Axes, min_ntrials: int, max_ntrials: int):\n",
    "    relevant_baseline, relevant_dynabo, relevant_pibo = select_relevant_data(baseline_data, dynabo_data, pibo_data, scenario, dataset, prior_kind)\n",
    "    relevant_baseline = fill_df(relevant_baseline, min_ntrials)\n",
    "    relevant_dynabo = fill_df(relevant_dynabo, min_ntrials)\n",
    "    relevant_pibo = fill_df(relevant_pibo, min_ntrials)\n",
    "\n",
    "    ax.plot(relevant_baseline[\"after_n_evaluations\"], relevant_baseline[\"avg_performance\"], label=\"baseline\")\n",
    "    ax.fill_between(relevant_baseline[\"after_n_evaluations\"], relevant_baseline[\"percentile_lower\"], relevant_baseline[\"percentile_upper\"], alpha=0.2)\n",
    "    ax.plot(relevant_dynabo[\"after_n_evaluations\"], relevant_dynabo[\"avg_performance\"], label=\"dynabo\")\n",
    "    ax.fill_between(relevant_dynabo[\"after_n_evaluations\"], relevant_dynabo[\"percentile_lower\"], relevant_dynabo[\"percentile_upper\"], alpha=0.2)\n",
    "    ax.plot(relevant_pibo[\"after_n_evaluations\"], relevant_pibo[\"avg_performance\"], label=\"pibo\")\n",
    "    ax.fill_between(relevant_pibo[\"after_n_evaluations\"], relevant_pibo[\"percentile_lower\"], relevant_pibo[\"percentile_upper\"], alpha=0.2)\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "def fill_df(iterator_df: pd.DataFrame, max_trials=200, x_axis_column: str = \"after_n_evaluations\"):\n",
    "    rows = []\n",
    "    for n_trials in sorted(iterator_df[\"after_n_evaluations\"].unique()):\n",
    "        if n_trials == 1:\n",
    "            relevant_df = iterator_df[iterator_df[\"after_n_evaluations\"] == n_trials]\n",
    "            after_n_evaluations = n_trials\n",
    "            after_runtime = relevant_df[\"after_runtime\"].max()\n",
    "            after_virtual_runtime = relevant_df[\"after_virtual_runtime\"].max()\n",
    "            after_reasoning_runtime = relevant_df[\"after_reasoning_runtime\"].max()\n",
    "            avg_performance = relevant_df[\"performance\"].mean()\n",
    "            std_performance = relevant_df[\"performance\"].std()\n",
    "            percentile_upper = np.percentile(relevant_df[\"performance\"], 95)\n",
    "            percentile_lower = np.percentile(relevant_df[\"performance\"], 5)\n",
    "            rows.append([after_n_evaluations, after_runtime, after_virtual_runtime, after_reasoning_runtime, avg_performance, std_performance, percentile_upper, percentile_lower])\n",
    "        else:\n",
    "            # Find row of last incumbent for each of the experiment_ids\n",
    "            last_incumbent_rows = []\n",
    "            for experiment_id in iterator_df[\"experiment_id\"].unique():\n",
    "                last = find_last(iterator_df, experiment_id, x_axis_column, n_trials)\n",
    "                last_incumbent_rows.append(last)\n",
    "            last_incumbent_df = pd.concat(last_incumbent_rows)\n",
    "            after_n_evaluations = n_trials\n",
    "            after_runtime = last_incumbent_df[\"after_runtime\"].max()\n",
    "            after_virtual_runtime = last_incumbent_df[\"after_virtual_runtime\"].max()\n",
    "            after_reasoning_runtime = last_incumbent_df[\"after_reasoning_runtime\"].max()\n",
    "            avg_performance = last_incumbent_df[\"performance\"].mean()\n",
    "            std_performance = last_incumbent_df[\"performance\"].std()\n",
    "            percentile_upper = np.percentile(last_incumbent_df[\"performance\"], 95)\n",
    "            percentile_lower = np.percentile(last_incumbent_df[\"performance\"], 5)\n",
    "            rows.append([after_n_evaluations, after_runtime, after_virtual_runtime, after_reasoning_runtime, avg_performance, std_performance, percentile_upper, percentile_lower])\n",
    "\n",
    "    new_df = pd.DataFrame(\n",
    "        rows, columns=[\"after_n_evaluations\", \"after_runtime\", \"after_virtual_runtime\", \"after_reasoning_runtime\", \"avg_performance\", \"std_performance\", \"percentile_upper\", \"percentile_lower\"]\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def find_last(df: pd.DataFrame, experiment_id: int, column: str, current: int):\n",
    "    last_trial = df[(df[\"experiment_id\"] == experiment_id) & (df[column] < current)]\n",
    "    if len(last_trial) == 0:\n",
    "        raise ValueError(\"No previous trial found\")\n",
    "    else:\n",
    "        last_column_value = df[(df[\"experiment_id\"] == experiment_id) & (df[column] < current)][column].max()\n",
    "        df = df[(df[\"experiment_id\"] == experiment_id) & (df[column] == last_column_value)]\n",
    "        return df\n",
    "\n",
    "\n",
    "def select_relevant_data(baseline_data: pd.DataFrame, dynabo_data: pd.DataFrame, pibo_data: pd.DataFrame, scenario: str, dataset: str, prior_kind):\n",
    "    relevant_baseline = baseline_data[(baseline_data[\"scenario\"] == scenario) & (baseline_data[\"dataset\"] == dataset)]\n",
    "    relevant_dynabo = dynabo_data[(dynabo_data[\"scenario\"] == scenario) & (dynabo_data[\"dataset\"] == dataset) & (dynabo_data[\"prior_kind\"] == prior_kind)]\n",
    "    relevant_pibo = pibo_data[(pibo_data[\"scenario\"] == scenario) & (pibo_data[\"dataset\"] == dataset) & (pibo_data[\"prior_kind\"] == prior_kind)]\n",
    "    return relevant_baseline, relevant_dynabo, relevant_pibo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = merge_df(baseline_table, baseline_incumbent, None)\n",
    "dynabo_df = merge_df(dynabo_table, dynabo_incumbent, dynabo_priors)\n",
    "pibo_df = merge_df(pibo_table, pibo_incumbent, pibo_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in baseline_df[\"dataset\"].unique():\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(15, 5))\n",
    "    plot_run(baseline_df, dynabo_df, pibo_df, \"lcbench\", dataset, \"good\", axs, min_ntrials=1, max_ntrials=200)\n",
    "    fig.legend()\n",
    "    fig.suptitle(dataset)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DynaBO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
